{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\FFF\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/LPRNet')\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/MTCNN')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from MTCNN import *\n",
    "from LPRNet_Test import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import easyocr\n",
    "harcascade = \"haarcascade_russian_plate_number.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 7\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     gray_blurred \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m     _, threshold \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(gray_blurred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY \u001b[38;5;241m+\u001b[39m cv2\u001b[38;5;241m.\u001b[39mTHRESH_OTSU)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "harcascade = \"haarcascade_russian_plate_number.xml\"\n",
    "plate_detector = cv2.CascadeClassifier(harcascade)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    _, threshold = cv2.threshold(gray_blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    morph_opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel)\n",
    "    contours, _ = cv2.findContours(morph_opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 500:\n",
    "            cv2.drawContours(frame, [contour], 0, (0, 255, 0), 2)\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.putText(frame, \"Contour\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Edge\", (x + w - 50, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Edges and Contours\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRNZE33 score:0.9841159525620041\n",
      "BR score:0.972437041308365\n",
      "BR score:0.9662023583554097\n",
      "BR score:0.9700844375614486\n",
      "QRMZE score:0.9662026206865605\n",
      "QRNZE score:0.9663727869921924\n",
      "QRMZE33 score:0.9776517165524792\n",
      "QRMZE33 score:0.9438834897844725\n",
      "BR score:0.9531953214294024\n",
      "BR score:0.9583258550228483\n",
      "BR score:0.9525173945730416\n",
      "BR score:0.9492163827354839\n",
      "BR score:0.9737054261814505\n",
      "BR score:0.9841737862660972\n",
      "BR score:0.9423103812862955\n",
      "BR score:0.9625841570390731\n",
      "BR score:0.9530812848651387\n",
      "QRMZE score:0.9897272592451043\n",
      "BR score:0.9728863728034635\n",
      "QRMZE3 score:0.9759694212078242\n",
      "BR score:0.9627923905414623\n",
      "BR score:0.9625855741160012\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(3, 640) # width\n",
    "cap.set(4, 480) #height\n",
    "\n",
    "min_area = 500\n",
    "count = 0\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "def calculate_blur(image, threshold=500):\n",
    "    \"\"\"\n",
    "    Calculate the blur level of an image and check if it meets the acceptance threshold.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        threshold (float): The minimum variance of Laplacian to accept the image as \"sharp\".\n",
    "    \n",
    "    Returns:\n",
    "        blur_level (float): The calculated variance of the Laplacian.\n",
    "        is_acceptable (bool): True if the image is sharp enough, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale for calculation\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Laplacian\n",
    "    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    \n",
    "    # Calculate the variance (a measure of sharpness)\n",
    "    blur_level = laplacian.var()\n",
    "    \n",
    "    # Check if the blur level meets the threshold\n",
    "    is_acceptable = blur_level > threshold\n",
    "    \n",
    "    return blur_level, is_acceptable\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    plate_cascade = cv2.CascadeClassifier(harcascade)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    plates = plate_cascade.detectMultiScale(img_gray, 1.1, 4)\n",
    "\n",
    "    for (x,y,w,h) in plates:\n",
    "        area = w * h\n",
    "\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "            plate_img = img[y: y+h, x:x+w]\n",
    "\n",
    "            _, acc = calculate_blur(plate_img)\n",
    "            #print(_)\n",
    "            if acc:\n",
    "                #print(_)\n",
    "                result = reader.readtext(img[y: y+h, x:x+w])\n",
    "                if(len(result) > 0 and result[0][2] >.94 ):\n",
    "                    cv2.putText(img, str(result[0][1]) + str(result[0][2]), (x,y-5), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 255), 2)\n",
    "\n",
    "                    print(str(result[0][1]) + \" score:\" + str(result[0][2]))\n",
    "                img_roi = img[y: y+h, x:x+w]\n",
    "                cv2.imshow(\"ROI\", img_roi)\n",
    "\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Result\", img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        cv2.imwrite(\"plates/scaned_img_\" + str(count) + \".jpg\", img_roi)\n",
    "        cv2.rectangle(img, (0,200), (640,300), (0,255,0), cv2.FILLED)\n",
    "        cv2.putText(img, \"Plate Saved\", (150, 265), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Results\",img)\n",
    "        cv2.waitKey(500)\n",
    "        count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 6.25M/6.25M [00:00<00:00, 11.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING  NMS time limit 2.050s exceeded\n",
      "0: 384x640 1 car, 1 truck, 80.8ms\n",
      "Speed: 60.5ms preprocess, 80.8ms inference, 7307.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m plates \u001b[38;5;241m=\u001b[39m yolo_plate_model(img)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(plates) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x,y,w,h) \u001b[38;5;129;01min\u001b[39;00m plates:\n\u001b[0;32m     74\u001b[0m         area \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m*\u001b[39m h\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m area \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50000\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/LPRNet')\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/MTCNN')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import argparse\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "#from MTCNN import *\n",
    "#from LPRNet_Test import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import easyocr\n",
    "harcascade = \"haarcascade_russian_plate_number.xml\"\n",
    "img = cv2.imread('car_1.png')\n",
    "\n",
    "reader = easyocr.Reader(['pt'], recog_network='latin_g2', gpu=True)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_blur(image, threshold=500):\n",
    "    \"\"\"\n",
    "    Calculate the blur level of an image and check if it meets the acceptance threshold.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        threshold (float): The minimum variance of Laplacian to accept the image as \"sharp\".\n",
    "    \n",
    "    Returns:\n",
    "        blur_level (float): The calculated variance of the Laplacian.\n",
    "        is_acceptable (bool): True if the image is sharp enough, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale for calculation\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Laplacian\n",
    "    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    \n",
    "    # Calculate the variance (a measure of sharpness)\n",
    "    blur_level = laplacian.var()\n",
    "    \n",
    "    # Check if the blur level meets the threshold\n",
    "    is_acceptable = blur_level > threshold\n",
    "    \n",
    "    return blur_level, is_acceptable\n",
    "\n",
    "\n",
    "plate_detector = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "plate_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if img is None:\n",
    "    print(\"Error: Image not found!\")\n",
    "else:\n",
    "    cv2.imshow(\"IMG\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "yolo_plate_model = YOLO(\"yolov8n.pt\")  \n",
    "#plates = plate_cascade.detectMultiScale(img_gray, 1.001, 4)\n",
    "#print(plates)\n",
    "plates = yolo_plate_model(img)\n",
    "if len(plates) > 0:\n",
    "    for (x,y,w,h) in plates:\n",
    "        area = w * h\n",
    "    \n",
    "        if area > 100 and area < 50000:\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "            plate_img = img[y: y+h, x:x+w]\n",
    "    \n",
    "            _, acc = calculate_blur(plate_img)\n",
    "            #print(_)\n",
    "            if acc:\n",
    "                #print(_)\n",
    "                result = reader.readtext(img[y: y+h, x:x+w])\n",
    "                if(len(result) > 0 and result[0][2] >.2):\n",
    "                    cv2.putText(img, str(result[0][1]) + str(result[0][2]), (x,y-5), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 255), 2)\n",
    "    \n",
    "                    print(str(result[0][1]) + \" score:\" + str(result[0][2]))\n",
    "                img_roi = img[y: y+h, x:x+w]\n",
    "                #cv2.imshow(\"ROI\", img_roi)\n",
    "                #cv2.waitKey(0)\n",
    "                #cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Result\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 2 Platess, 51.9ms\n",
      "Speed: 5.0ms preprocess, 51.9ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "[([[31, 17], [175, 17], [175, 57], [31, 57]], '[RSGK', 0.2538915877464017)]\n",
      "[RSGK score:0.2538915877464017\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/LPRNet')\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/MTCNN')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import argparse\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "#from MTCNN import *\n",
    "#from LPRNet_Test import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import easyocr\n",
    "harcascade = \"haarcascade_russian_plate_number.xml\"\n",
    "img = cv2.imread('2.png')\n",
    "\n",
    "reader = easyocr.Reader(['pt'], recog_network='latin_g2', gpu=True)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_blur(image, threshold=500):\n",
    "    \"\"\"\n",
    "    Calculate the blur level of an image and check if it meets the acceptance threshold.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        threshold (float): The minimum variance of Laplacian to accept the image as \"sharp\".\n",
    "    \n",
    "    Returns:\n",
    "        blur_level (float): The calculated variance of the Laplacian.\n",
    "        is_acceptable (bool): True if the image is sharp enough, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale for calculation\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Laplacian\n",
    "    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    \n",
    "    # Calculate the variance (a measure of sharpness)\n",
    "    blur_level = laplacian.var()\n",
    "    \n",
    "    # Check if the blur level meets the threshold\n",
    "    is_acceptable = blur_level > threshold\n",
    "    \n",
    "    return blur_level, is_acceptable\n",
    "\n",
    "\n",
    "plate_detector = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "plate_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if img is None:\n",
    "    print(\"Error: Image not found!\")\n",
    "else:\n",
    "    cv2.imshow(\"IMG\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "yolo_plate_model = YOLO(\"best_plate.pt\")  \n",
    "#plates = plate_cascade.detectMultiScale(img_gray, 1.001, 4)\n",
    "#print(plates)\n",
    "results = yolo_plate_model(img)\n",
    "if len(plates) > 0:\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x, y, w, h = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            cv2.rectangle(img, (x-10, y-10), (w+10, h+10), (0, 255, 0), 2)\n",
    "            plate_img = img[y: y+h, x:x+w]\n",
    "            _, acc = calculate_blur(plate_img)\n",
    "            #print(_)\n",
    "            if acc:\n",
    "                #print(_)\n",
    "                result = reader.readtext(img[max(y-20, 0): y+h+20, max(x-20,0):x+w+20])\n",
    "                if(len(result) > 0 and result[0][2] >.2):\n",
    "                    cv2.putText(img, str(result[0][1]) + str(result[0][2]), (x,y-5), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 255), 2)\n",
    "                    print(result)\n",
    "                    print(str(result[0][1]) + \" score:\" + str(result[0][2]))\n",
    "                img_roi = img[y: y+h, x:x+w]\n",
    "                #cv2.imshow(\"ROI\", img_roi)\n",
    "                #cv2.waitKey(0)\n",
    "                #cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Result\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 2 Platess, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float expected at most 1 argument, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m result \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(img[\u001b[38;5;28mmax\u001b[39m(y, \u001b[38;5;241m0\u001b[39m): y\u001b[38;5;241m+\u001b[39mh, \u001b[38;5;28mmax\u001b[39m(x,\u001b[38;5;241m0\u001b[39m):x\u001b[38;5;241m+\u001b[39mw])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m.2\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, \u001b[38;5;28mstr\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m , (x,y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX_SMALL, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m score:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: float expected at most 1 argument, got 2"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/LPRNet')\n",
    "sys.path.append('./License_Plate_Detection_Pytorch/MTCNN')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import argparse\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "#from MTCNN import *\n",
    "#from LPRNet_Test import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import easyocr\n",
    "harcascade = \"haarcascade_russian_plate_number.xml\"\n",
    "img = cv2.imread('2.png')\n",
    "\n",
    "reader = easyocr.Reader(\n",
    "    lang_list=['en', 'fr'],      # English & French\n",
    "    gpu=True,                    # Use GPU if available\n",
    "#    model_storage_directory='./models',  # Custom model storage\n",
    "    recog_network='english_g2',  # More accurate English model\n",
    "    download_enabled=True        # Allow model download if needed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_blur(image, threshold=500):\n",
    "    \"\"\"\n",
    "    Calculate the blur level of an image and check if it meets the acceptance threshold.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        threshold (float): The minimum variance of Laplacian to accept the image as \"sharp\".\n",
    "    \n",
    "    Returns:\n",
    "        blur_level (float): The calculated variance of the Laplacian.\n",
    "        is_acceptable (bool): True if the image is sharp enough, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale for calculation\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Laplacian\n",
    "    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    \n",
    "    # Calculate the variance (a measure of sharpness)\n",
    "    blur_level = laplacian.var()\n",
    "    \n",
    "    # Check if the blur level meets the threshold\n",
    "    is_acceptable = blur_level > threshold\n",
    "    \n",
    "    return blur_level, is_acceptable\n",
    "\n",
    "\n",
    "plate_detector = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "plate_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + harcascade)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if img is None:\n",
    "    print(\"Error: Image not found!\")\n",
    "else:\n",
    "    cv2.imshow(\"IMG\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "yolo_plate_model = YOLO(\"best_plate.pt\")  \n",
    "#plates = plate_cascade.detectMultiScale(img_gray, 1.001, 4)\n",
    "#print(plates)\n",
    "results = yolo_plate_model(img)\n",
    "if len(plates) > 0:\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x, y, w, h = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            cv2.rectangle(img, (x-10, y-10), (w+10, h+10), (0, 255, 0), 2)\n",
    "            plate_img = img[y: y+h, x:x+w]\n",
    "            _, acc = calculate_blur(plate_img)\n",
    "            #print(_)\n",
    "            if acc:\n",
    "                #print(_)\n",
    "                result = reader.readtext(img[max(y, 0): y+h, max(x,0):x+w])\n",
    "                if(len(result) > 0 and result[0][2] >.2):\n",
    "                    cv2.putText(img, str(result[0][1]) + \" \"+str(float(result[0][2]*100, 2)) + \"%\" , (x,y-5), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 255), 2)\n",
    "                    print(result)\n",
    "                    print(str(result[0][1]) + \" score:\" + str(result[0][2]))\n",
    "                img_roi = img[y: y+h, x:x+w]\n",
    "                #cv2.imshow(\"ROI\", img_roi)\n",
    "                #cv2.waitKey(0)\n",
    "                #cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Result\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "harcascade = \"haarcascade_russian_plate_number.xml\"\n",
    "plate_detector = cv2.CascadeClassifier(harcascade)\n",
    "\n",
    "\n",
    "frame = cv2.imread('car_1.png')\n",
    "if frame is None:\n",
    "    print(\"Error: Image not found!\")\n",
    "else:\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    _, threshold = cv2.threshold(gray_blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    morph_opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel)\n",
    "    contours, _ = cv2.findContours(morph_opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 500:\n",
    "            cv2.drawContours(frame, [contour], 0, (0, 255, 0), 2)\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.putText(frame, \"Contour\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Edge\", (x + w - 50, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Edges and Contours\", frame)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
